---
title: "Ethereum Token Analysis"
author: "Pushpita Panigrahi(pxp171530), Akash Chand(axc173730), Siddharth Swarup Panda(ssp171730)"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

###Blockchain Technology
Block chain is a growing list of records, called blocks, which are linked using cryptography (cryptography is the practice and study of techniques for secure communication in the presence of third parties) .Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data. *"The blockchain is an incorruptible digital ledger of economic transactions that can be programmed to record not just financial transactions but virtually everything of value."* It is an open, distributed ledger (a consensus of replicated, shared, and synchronized digital data geographically spread across multiple sites) that can record transactions between two parties efficiently and in a verifiable and permanent way". A blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered without alterating all its subsequent blocks, which requires consensus of the network majority. 

###Ethereum
Ethereum is a distributed public block chain network that focuses on running programming code of any decentralized application. More simply, it is a platform for sharing information across the globe that cannot be manipulated or changed. 
Ether, a decentralized digital currency, also known as ETH is a cryptocurrency whose blockchain is generated by the Ethereum platform. In addition to being a tradeable cryptocurrency, ether powers the Ethereum network by paying for transaction fees and computational services. As with other cryptocurrencies, the validity of each ether is provided by a blockchain, which is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Unlike Bitcoin, Ethereum operates using accounts and balances in a manner called state transitions.

**ERC-20** is a technical standard used for smart contracts on the Ethereum blockchain for implementing tokens. ERC stands for Ethereum Request for Comment, and 20 is the number that was assigned to this request.ERC-20 defines a common list of rules for Ethereum tokens to follow within the larger Ethereum ecosystem, allowing developers to accurately predict interaction between tokens. These rules include how the tokens are transferred between addresses and how data within each token is accessed

### BNB Token
We chose Binance coin(networkbnbTX) token as our dataset. There are typically two different types of exchanges: the ones that deal with fiat currency and the ones that deal purely in crypto. The latter one even though they are small now, it is expected that pure crypto exchanges will  have a bigger impact than fiat based exchanges in the
near future. They will play an ever more important role in world finance and this new paradigm is called as Binance; Binary Finance.
Features of Binance include :
1) Safety Stability - Multi-tier & multi-cluster system architecture
2) High Performance - Capable of processing 1,400,000 orders per second
3) High Liquidity - Abundant resources and partners
4) All Devices Covered - Web, Android, iOS, Mobile Web, Windows, macOS
5) Multilingual Support - Support and FAQs available in multiple languages
6) Multiple-Coin Support - BTC, ETH, LTC, BNB

## Steps

### Data preparation and preprocessing
The networkbnbTX file is read into a dataframe. We get the total supply and possible sub units of BNB token in market from www.coinmarketcap.com. Token edge files have following row structure: **fromNodeID, toNodeID, unixTime, tokenAmount**. Each row implies that *fromNodeID* sold tokenAmount of the token to *toNodeID* at time *unixTime*. *fromNodeID* and *toNodeID* are ids for people who invest in the token in real life. Each person can use multiple ids and 2 ids can sell/buy tokens multiple times with multiple amounts. This makes our network a weighted, directed, multiedge graph. Below is the sample data from the network file:

```{r}
library(plyr)
library(ggplot2) 
library(fitdistrplus)
require(plyr)
library(grid)
```

```{r}
file <-'networkbnbTX.txt'
col_names <- c("FROMNODE","TONODE","DATE","TOKENAMOUNT")
mydata <- read.csv( file, header = FALSE, sep = " ", dec = ".", col.names = col_names)
mydata$DATE <- as.Date(as.POSIXct(as.numeric(mydata$DATE), origin = '1970-01-01', tz = 'GMT'))
amounts <- mydata[4]

totalSupply <- 192443301
subUnits <- 18
totalAmount <- totalSupply * (10 ^ subUnits)
#head(mydata)
```
 
The price file for BNB token is read into a dataframe which contains the open, clase, max and min price for the token foe each day. The row structure of the file is **Date, Open, High, Low, Close, Volume, MarketCap**. *Open* and *close* are the prices of the specific token at each day. *Volume* gives total bought/sold tokens and *MarketCap* gives the market valuation at each day. Below is a sample data of the price file: 

```{r}
pricefile <-'bnb.txt'
col_names <- c("Date","Open","High","Low","Close","Volume","MarketCap")
myPrices <- read.csv( pricefile , header = TRUE, sep = "\t", dec = ".", col.names = col_names)
myPrices$Date <- format(as.Date(myPrices$Date, format = "%m/%d/%Y"), "%Y-%m-%d")
#head(myPrices)
```

### Preprocessing
The preprocessing step involves removal of fraudulent transactions which might affect the distribution estimate negatively. The total supply of the networkbnb token is 192443301 (quoted from etherscan.io) and the range of subunits for the token is 18 decimal units. Thus any transaction having that attempts to log a value, i.e, *tokenAmount*, greater than the product of total supply and subunits is deemed as fraudulent.
**Result: **The token networkbnb does not have any fraudulent transactions.

```{r}
temp <- which(mydata< totalAmount)
#print meta data 
message('Maximum allowed amount : ', totalAmount)
count <- 0
outliers <- 0
for( a in 1:nrow(amounts)){
  if( a > totalAmount){
    outliers <- outliers + 1
  }
  else{
    count <- count + 1
  }
}
message('Number of fradulent transactions : ',outliers)
message('Number of valid amounts : ',count)
```

### Package used to fit distributions
For approximating distributions, descdist R function is used which provides a Cullen and Frey skewness-kurtosis plot. Bootstrapped samples of the data have been used in order to consider the uncertainty of the estimated skewness and kurtosis values which are higher order moments. Skewness is a measure of symmetry while kurtosis is the measure of the combined weight of distribution's tails relative to the center of the distribution, which is why both the statistics are dependable to estimate the distribution of the data.

### Function to remove outliers
The data points which fall outside 2.5 times the inter-quartile range are considered as outliers and are removed from the dataset. We tested our experiments with values 1, 1,5, 2, 2.5, 3 times the IQR and found the best results with 2.5 IQR value. We believe this is because we need a significant number of data points to fit our distributions.
```{r}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 2.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

## Study 1:

The aim of this experiment is to find a distribution of how many times the user id transacts in BNB token. We find the frequencies of buys and sells separately for each user and try to fit a distribution for the frequencies. This helps us understand the transaction patterns user wise for our token. Below is the statistics of the frequency distribution and visualisation of the count of frequencies. This helps us understand how likely it is to retain users using the token. 

### Calculating and plotting selling frequency
```{r pressure, echo=FALSE}
countFromDf <- count(mydata, "FROMNODE")
countFromFf <- count(countFromDf, "freq")
colnames(countFromFf) <- c("Users_Count", "Sell_Count")
plot(countFromFf$Users_Count, countFromFf$Sell_Count)
```

### Removing outliers and summarizing data
```{r}
newSet1 <- remove_outliers(countFromFf$Sell_Count)
maxCount1 = max(newSet1[complete.cases(newSet1)])
minCount1 = min(newSet1[complete.cases(newSet1)])
countFromFf <- subset(countFromFf, Sell_Count<maxCount1 & Sell_Count>minCount1)
#head(countFromFf)
#descdist(countFromFf$Sell_Count, boot= 500, discrete=TRUE)
descdist(countFromFf$Sell_Count, boot= 500)
```

### Approximating the selling distributions
From the above Cullen and Frey graph we could narrow down our distribution selection to Weibull, lognormal, gamma and poisson.
```{r}
distributionFit_Seller_pois <- fitdist(countFromFf$Sell_Count, "pois", method ="mle")
distributionFit_Seller_wb <- fitdist(countFromFf$Sell_Count, "weibull", method ="mle")
distributionFit_Seller_ln <- fitdist(countFromFf$Sell_Count, "lnorm", method ="mle")
distributionFit_Seller_gm <- fitdist(countFromFf$Sell_Count, "gamma" ,method="mme")
#distributionFit_Seller_wb$loglik
#plot(distributionFit_Seller_wb)
#distributionFit_Seller_pois$loglik
#plot(distributionFit_Seller_pois)
#distributionFit_Seller_ln$loglik
#plot(distributionFit_Seller_ln)
#distributionFit_Seller_gm$loglik
#plot(distributionFit_Seller_gm)
```

### Calculating and plotting buying frequency
```{r}
countToDf <- count(mydata, "TONODE")
countToFf <- count(countToDf, "freq")
colnames(countToFf) <- c("Users_Count", "Buy_Count")
plot(countToFf$Users_Count, countToFf$Buy_Count)
```

### Removing outliers and summarizing data
```{r}
newSet2 <- remove_outliers(countToFf$Buy_Count)
maxCount2 = max(newSet2[complete.cases(newSet2)])
minCount2 = min(newSet2[complete.cases(newSet2)])
countToFf <- subset(countToFf, Buy_Count<maxCount2 & Buy_Count>minCount2)
#descdist(countToFf$Buy_Count, boot= 500, discrete=TRUE, graph=FALSE)
descdist(countToFf$Buy_Count, boot=500, graph=FALSE)
```

### Approximating the buying distributions
```{r}
distributionFit_Buyer_pois <- fitdist(countToFf$Buy_Count, "pois", method ="mle")
distributionFit_Buyer_wb <- fitdist(countToFf$Buy_Count, "weibull", method ="mle")
distributionFit_Buyer_ln <- fitdist(countToFf$Buy_Count, "lnorm", method ="mle")
distributionFit_Buyer_gm <- fitdist(countToFf$Buy_Count, "gamma", method ="mme")
#distributionFit_Buyer_pois$loglik
#plot(distributionFit_Buyer_pois)
#distributionFit_Buyer_wb$loglik
#plot(distributionFit_Buyer_wb)
#distributionFit_Buyer_ln$loglik
#plot(distributionFit_Buyer_ln)
#distributionFit_Buyer_gm$loglik
#plot(distributionFit_Buyer_gm)
```

### Study 1: Conclusion
From the above graph estimates, both buy and sell frequency for our dataset follows LOG-NORMAL distribution as the log likelihood value is maximum and standard error is least and the emperical distribution curve follows the theoritical distribution curve for the log-normal graph most accurately.


## Study 3:

We find the most active users in BNB token and try to fit a distribution for their activities among all the tokens throughout given dataset. We take into consideration anyone who is buying or selling the token as we are interested in all activities.

### Getting the active users
We first find out the most active users for our token. Active users are selected as those users who buy/sell BNB token more than the average count of all users buying/selling BNB token. This is done to get enough data points for fitting the distrbution later on.
```{r}
allUsers <- append(mydata$TONODE, mydata$FROMNODE)
allUsers <- data.frame(allUsers)
colnames(allUsers) <- c("USERS")

usersFreq <- count(allUsers, "USERS")
meanFreq <- mean(usersFreq$freq)
activeUsers <- usersFreq[(usersFreq$freq>meanFreq),]
```

### Reading all other token data
We go thorugh all the other tokens in given dataset and find out how many tokens each user buys/sells
```{r}
col_names <- c("FROMNODE","TONODE","DATE","TOKENAMOUNT")
fpath<-"/Users/pushpitapanigrahi/Desktop/PushpitaFiles/Study/4.StatsForDS/Proj1/Ethereum token graphs"
files <- list.files(path=fpath, pattern="*.txt", full.names=TRUE, recursive=FALSE)

uniqueUsersForAllTokens <- list() #For every token a user transacts in, there is one entry of the userId in this list
for(i in 1:length(files)){
  t <- data.frame(read.csv( files[i], header = FALSE, sep = " ", dec = ".", col.names = col_names))
  tusers <- unique(append(t$FROMNODE, t$TONODE))
  uniqueUsersForAllTokens <- append(uniqueUsersForAllTokens,tusers)
}
usersFromAllTokens <- do.call(rbind.data.frame, uniqueUsersForAllTokens)
colnames(usersFromAllTokens)<-c("USERID")
```

### Counting the number of tokens per userid
We now reverse the frequency count, to find how many unique tokens each user id is transacting in. Below is a snipet of resulting data:
```{r}
userTokenCount <- data.frame(table(usersFromAllTokens$USERID[usersFromAllTokens$USERID %in% activeUsers$USERS]))
colnames(userTokenCount) <-  c("USERID", "COUNT")
#head(userTokenCount)
freqOfTokenCount <- count(userTokenCount, "COUNT")
colnames(freqOfTokenCount) <- c("Users_Count", "Freq_Count")
#head(freqOfTokenCount)
plot(freqOfTokenCount$Users_Count, freqOfTokenCount$Freq_Count)
```

### Removing outliers and summarizing unique token counts for the active users
We use the same Culley and Fray graph software to find the distributions our data approximately fits into.
```{r}
newSet3 <- remove_outliers(freqOfTokenCount$Freq_Count)
maxCount3 = max(newSet3[complete.cases(newSet3)])
minCount3 = min(newSet3[complete.cases(newSet3)])
freqOfTokenCount <- subset(freqOfTokenCount, Freq_Count<maxCount3 & Freq_Count>minCount3)
#descdist(freqOfTokenCount$Freq_Count, boot= 500, discrete=TRUE, graph=FALSE)
descdist(freqOfTokenCount$Freq_Count, boot= 500, graph=FALSE)
```

### Fitting the distrubution to find the closest fit
```{r}
distributionFit_Count_pois <- fitdist(freqOfTokenCount$Freq_Count, "pois", method ="mle")
distributionFit_Count_wb <- fitdist(freqOfTokenCount$Freq_Count, "weibull", method ="mle")
distributionFit_Count_ln <- fitdist(freqOfTokenCount$Freq_Count, "lnorm", method ="mle")
distributionFit_Count_gm <- fitdist(freqOfTokenCount$Freq_Count, "gamma" ,method="mme")
#distributionFit_Count_wb$loglik
#plot(distributionFit_Count_wb)
#distributionFit_Seller_pois$loglik
#plot(distributionFit_Count_pois)
#distributionFit_Seller_ln$loglik
#plot(distributionFit_Count_ln)
#distributionFit_Seller_gm$loglik
#plot(distributionFit_Count_gm)
```

### Study 3: Conclusion 
From the above graph estimates, the number of unique tokens in our dataset in which the most active users of BNB token transact, follows a weibull distribution as the log likelihood value is maximum, also the emperical distribution curve follows the theoritical distribution curve for the log-normal graph most accurately.




## Study 2: 

Here the aim is to select a feature and select a criteria for creating layers of transaction and finding the correlation of price data for BNB token among the layers. We find the correlation between the unique number of buyers(feature) in each day(layer) to the token opening price for the day. We select the layers to be a day as that is the minimum division available within the dataset. This study will help understanding the degree of correlation between how user activities model with respect to the opening price of the token in each day. 

### Studying distribution of the opening price . 
Study the pattern for opening price values on each day for BNB token. We do not see any outliers in this data.
```{r}
timePrices <- subset(myPrices, select=c("Date","Open", "Close"))
timePrices$Date <- as.Date(timePrices$Date, "%Y-%m-%d")
timePrices <- unique(timePrices)
#summary(timePrices)
plot(timePrices$Date, timePrices$Open, main = "Opening prices VS date", xlab = "Date", ylab="Open price")
```

### Studying the distribution of number of unique buyers each day. 
Study the pattern of how many unique buyers are there for our token each day. We see outliers in this data. 
```{r}
timeBuyFreq <- ddply(mydata, .(DATE), mutate, count = length(unique(TONODE)))
timeBuyFreq <- subset(timeBuyFreq, select=c("DATE", "count"))
timeBuyFreq$DATE <- as.Date(timeBuyFreq$DATE, "%Y-%m-%d")
timeBuyFreq <- unique(timeBuyFreq)
#summary(timeBuyFreq)
outliers <- boxplot(timeBuyFreq$count, main="Unique buyer count distribution", ylab="unique buyer count")$out
```
```{r}

```

We see the summary of the outliers and plot the data with and without the outliers.
```{r}
#summary(outliers)
plot( timeBuyFreq$DATE, timeBuyFreq$count ,ylim=c(0, 633), main = "Unique buyer count VS date", xlab = "Date", ylab="Unique buyer count")
```

### Combine opening price and unique buyer count for each day
We remove the outliers and merge the price and buyer counts to find the pearson correlation between the two fields with each day being a layer.
```{r}
priceSellForEachDay <- merge(x=timePrices, y=timeBuyFreq, by.x=c("Date"), by.y = c("DATE"))
newSet <- remove_outliers(priceSellForEachDay$count)
maxCount = max(newSet[complete.cases(newSet)])
minCount = min(newSet[complete.cases(newSet)])
priceSellForEachDay <- subset(priceSellForEachDay, count<maxCount & count>minCount)
#head(priceSellForEachDay)
cor(priceSellForEachDay$Open, priceSellForEachDay$count, method=c("pearson"))
```

### Study 2: Conclusion
We find a very strong positive correlation between the number of people buying BNB token in a day to the price of the token that day. This menas that user decisions get highly effected by the opening price of bnb token on that day.
So we combine both plots to visualize the correlation. 

```{r}
#' Create the two plots.
p1 <- ggplot(priceSellForEachDay, aes(Date, count)) + geom_line() + theme_minimal() + 
      theme(axis.title.x = element_blank(), axis.text.x = element_blank())
p2 <- ggplot(priceSellForEachDay,aes(Date, Open)) + geom_bar(stat="identity") + theme_minimal() + 
      theme(axis.title.x = element_blank(),axis.text.x = element_text(angle=90))
grid.newpage()
grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), size = "last"))
```

### Compare opening price and number of transactions for each day
```{r}
timeBuyFreq1 <- ddply(mydata, .(DATE), mutate, count = length(TONODE))
#head(timeBuyFreq1)
timeBuyFreq1 <- subset(timeBuyFreq1, select=c("DATE", "count"))
timeBuyFreq1$DATE <- as.Date(timeBuyFreq1$DATE, "%Y-%m-%d")
timeBuyFreq1 <- unique(timeBuyFreq1)
priceSellForEachDay1 <- merge(x=timePrices, y=timeBuyFreq1, by.x=c("Date"), by.y = c("DATE"))
newSet1 <- remove_outliers(priceSellForEachDay1$count)
maxCount1 = max(newSet1[complete.cases(newSet1)])
minCount1 = min(newSet1[complete.cases(newSet1)])
priceSellForEachDay1 <- subset(priceSellForEachDay1, count<maxCount1 & count>minCount1)
cor(priceSellForEachDay1$Open, priceSellForEachDay1$count, method=c("pearson"))
```


### Compare opening price and closing price of previous day for each day
```{r}
y2=list()
y2 <- append(y2,0)
for(i in 2:nrow(priceSellForEachDay)){
  y2 <- append(y2,priceSellForEachDay$Close[i-1])
}
priceSellForEachDay$ClosePrev <- unlist(y2, use.names=FALSE)
```

### Merge all three regressors, and calculate price return, simple price return is given as  $$P_t − (P_t − 1)/P_t − 1$$
```{r}
corDataSet <- merge(x= priceSellForEachDay, y=priceSellForEachDay1, by.x=c("Date"), by.y=c("Date"))
y=list()
for(i in 1:nrow(corDataSet)-1){
  y <- append(y,(corDataSet$Open.x[i]-corDataSet$Open.x[i+1])/corDataSet$Open.x[i+1])
}
y <- append(y,0)
corDataSet$priceReturn <- unlist(y, use.names=FALSE)
```

### Building the prediction model
After studying the linear relationship pictorially in the scatter plot and by computing correlations, we build the linear model.We build a linear model using all 3 regressors and entire dataset considering each day as one layer. 
```{r}
linearMod <- lm( priceReturn ~ count.x + count.y + ClosePrev, data=corDataSet) 
summary(linearMod)
AIC(linearMod)
plot(residuals(linearMod))
```

### Summary and Conclusion 

